	1254
	<!--	1
!=	3
""	6
"".	4
"$HADOOP_CLASSPATH"	1
"$JAVA_HOME"	2
"$YARN_HEAPSIZE"	1
"$YARN_LOGFILE"	1
"$YARN_LOG_DIR"	1
"$YARN_POLICYFILE"	1
"*"	18
"AS	25
"Error:	1
"License");	25
"alice,bob	18
"console"	1
"dfs"	3
"hadoop.root.logger".	1
"jks".	4
"jvm"	3
"mapred"	3
"rpc"	3
"run	1
"ugi"	3
"x"	1
"x$JAVA_LIBRARY_PATH"	1
#	381
#!/bin/bash	2
###	4
#*.sink.ganglia.dmax=jvm.metrics.threadsBlocked=70,jvm.metrics.memHeapUsedM=40	1
#*.sink.ganglia.slope=jvm.metrics.gcCount=zero,jvm.metrics.memHeapUsedM=both	1
#*.sink.ganglia.tagsForPrefix.dfs=	1
#*.sink.ganglia.tagsForPrefix.jvm=ProcesName	1
#*.sink.ganglia.tagsForPrefix.mapred=	1
#*.sink.ganglia.tagsForPrefix.rpc=	1
#A	1
#Default	1
#HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData	1
#Security	1
#The	1
#datanode.sink.file.filename=datanode-metrics.out	1
#datanode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#dfs.class=org.apache.hadoop.metrics.file.FileContext	1
#dfs.fileName=/tmp/dfsmetrics.log	1
#dfs.period=10	1
#echo	1
#export	15
#jobhistoryserver.sink.file.filename=jobhistoryserver-metrics.out	1
#jobhistoryserver.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#jvm.class=org.apache.hadoop.metrics.file.FileContext	1
#jvm.class=org.apache.hadoop.metrics.spi.NullContext	1
#jvm.fileName=/tmp/jvmmetrics.log	1
#jvm.period=10	1
#log4j.additivity.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=false	1
#log4j.appender.DRFA.MaxBackupIndex=30	1
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HSAUDIT.DatePattern=.yyyy-MM-dd	1
#log4j.appender.HSAUDIT.File=${hadoop.log.dir}/hs-audit.log	1
#log4j.appender.HSAUDIT.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.HSAUDIT.layout=org.apache.log4j.PatternLayout	1
#log4j.appender.HSAUDIT=org.apache.log4j.DailyRollingFileAppender	1
#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601}	1
#log4j.appender.datanoderequestlog.Filename=${hadoop.log.dir}/jetty-datanode-yyyy_mm_dd.log	1
#log4j.appender.datanoderequestlog.RetainDays=3	1
#log4j.appender.datanoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.jobhistoryrequestlog.Filename=${hadoop.log.dir}/jetty-jobhistory-yyyy_mm_dd.log	1
#log4j.appender.jobhistoryrequestlog.RetainDays=3	1
#log4j.appender.jobhistoryrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.namenoderequestlog.Filename=${hadoop.log.dir}/jetty-namenode-yyyy_mm_dd.log	1
#log4j.appender.namenoderequestlog.RetainDays=3	1
#log4j.appender.namenoderequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.nodemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-nodemanager-yyyy_mm_dd.log	1
#log4j.appender.nodemanagerrequestlog.RetainDays=3	1
#log4j.appender.nodemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.appender.resourcemanagerrequestlog.Filename=${hadoop.log.dir}/jetty-resourcemanager-yyyy_mm_dd.log	1
#log4j.appender.resourcemanagerrequestlog.RetainDays=3	1
#log4j.appender.resourcemanagerrequestlog=org.apache.hadoop.http.HttpRequestLogAppender	1
#log4j.logger.BlockStateChange=WARN	1
#log4j.logger.http.requests.datanode=INFO,datanoderequestlog	1
#log4j.logger.http.requests.jobhistory=INFO,jobhistoryrequestlog	1
#log4j.logger.http.requests.namenode=INFO,namenoderequestlog	1
#log4j.logger.http.requests.nodemanager=INFO,nodemanagerrequestlog	1
#log4j.logger.http.requests.resourcemanager=INFO,resourcemanagerrequestlog	1
#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG	1
#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG	1
#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG	1
#log4j.logger.org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger=${mapreduce.hs.audit.logger}	1
#mapred.class=org.apache.hadoop.metrics.file.FileContext	1
#mapred.fileName=/tmp/mrmetrics.log	1
#mapred.period=10	1
#mapreduce.hs.audit.logger=INFO,HSAUDIT	1
#mrappmaster.sink.file.filename=mrappmaster-metrics.out	1
#mrappmaster.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#namenode.sink.*.period=8	1
#namenode.sink.file.filename=namenode-metrics.out	1
#namenode.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#nodemanager.sink.file.filename=nodemanager-metrics.out	1
#nodemanager.sink.file_jvm.class=org.apache.hadoop.metrics2.sink.FileSink	1
#nodemanager.sink.file_jvm.context=jvm	1
#nodemanager.sink.file_jvm.filename=nodemanager-jvm-metrics.out	1
#nodemanager.sink.file_mapred.class=org.apache.hadoop.metrics2.sink.FileSink	1
#nodemanager.sink.file_mapred.context=mapred	1
#nodemanager.sink.file_mapred.filename=nodemanager-mapred-metrics.out	1
#nodemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#resourcemanager.sink.file.filename=resourcemanager-metrics.out	1
#resourcemanager.sink.ganglia.servers=yourgangliahost_1:8649,yourgangliahost_2:8649	1
#rpc.class=org.apache.hadoop.metrics.file.FileContext	1
#rpc.fileName=/tmp/rpcmetrics.log	1
#rpc.period=10	1
#ugi.class=org.apache.hadoop.metrics.file.FileContext	1
#ugi.fileName=/tmp/ugimetrics.log	1
#ugi.period=10	1
#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY	1
$0	1
$HADOOP_CLIENT_OPTS"	1
$HADOOP_DATANODE_OPTS"	1
$HADOOP_HOME/contrib/capacity-scheduler/*.jar;	1
$HADOOP_HOME/logs	1
$HADOOP_JAVA_PLATFORM_OPTS"	1
$HADOOP_MAPRED_HOME/logs	1
$HADOOP_NAMENODE_OPTS"	1
$HADOOP_PORTMAP_OPTS"	1
$HADOOP_SECONDARYNAMENODE_OPTS"	1
$JAVA_HOME"	1
$USER	2
${HTTPFS_HTTP_PORT}	1
${KMS_HTTP_PORT}	1
%-5p	3
%5p	2
%HADOOP_CLIENT_OPTS%	1
%HADOOP_DATANODE_OPTS%	1
%HADOOP_HOME%/logs	1
%HADOOP_HOME%\contrib\capacity-scheduler	1
%HADOOP_JAVA_PLATFORM_OPTS%"	1
%HADOOP_NAMENODE_OPTS%	1
%HADOOP_SECONDARYNAMENODE_OPTS%	1
%USERNAME%	1
%X{op}	2
%YARN_HEAPSIZE%	1
%c:	5
%c{1}	2
%c{2}	2
%c{2}:	6
%m%n	17
%p	11
&	2
&quot;kerberos&quot;.	1
&quot;simple&quot;	1
'	3
'${httpfs.home}/logs'	1
'${kms.home}/logs'	1
'(i.e.	2
'*'	1
'*',	2
'.	1
':'	1
'HTTP/'	1
'httpfs.log.dir'	1
'kms.log.dir'	1
'none'	1
'queue'	1
'queues'	1
'random'	1
'sasl'	1
'string'	1
'zookeeper'	2
'zookeeper'.	1
(	13
(%F:%M(%L))	2
(10	2
(ASF)	12
(Hadoop	1
(Kerberos).	1
(default),	1
(former)	2
(fs,	2
(in	1
(latter)	2
(resource	2
(root	1
(specified	1
(the	25
)	13
**MUST	1
**MUST**	1
*.period=10	1
*.sink.file.class=org.apache.hadoop.metrics2.sink.FileSink	1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30	1
*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31	1
*.sink.ganglia.period=10	1
*.sink.ganglia.supportsparse=true	1
+	2
-	9
-->	43
-Dhadoop.home.dir=%HADOOP_YARN_HOME%	1
-Dhadoop.log.dir=$YARN_LOG_DIR"	1
-Dhadoop.log.dir=%YARN_LOG_DIR%	1
-Dhadoop.log.file=$YARN_LOGFILE"	1
-Dhadoop.log.file=%YARN_LOGFILE%	1
-Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}"	1
-Dhadoop.root.logger=%YARN_ROOT_LOGGER%	1
-Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}	2
-Dhdfs.audit.logger=%HDFS_AUDIT_LOGGER%	2
-Djava.library.path=$JAVA_LIBRARY_PATH"	1
-Djava.library.path=%JAVA_LIBRARY_PATH%	1
-Djava.net.preferIPv4Stack=true	1
-Djava.net.preferIPv4Stack=true"	1
-Dyarn.home.dir=$YARN_COMMON_HOME"	1
-Dyarn.home.dir=%HADOOP_YARN_HOME%	1
-Dyarn.id.str=$YARN_IDENT_STRING"	1
-Dyarn.id.str=%YARN_IDENT_STRING%	1
-Dyarn.log.dir=$YARN_LOG_DIR"	1
-Dyarn.log.dir=%YARN_LOG_DIR%	1
-Dyarn.log.file=$YARN_LOGFILE"	1
-Dyarn.log.file=%YARN_LOGFILE%	1
-Dyarn.policy.file=$YARN_POLICYFILE"	1
-Dyarn.policy.file=%YARN_POLICYFILE%	1
-Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}"	1
-Dyarn.root.logger=%YARN_ROOT_LOGGER%	1
-Xmx1000m,	3
-f`	1
/tmp	3
0.	1
0.0	1
1	1
1.0.	1
1000.	5
10000	2
1`	2
2.0	25
3.0	3
3.1	3
30-day	1
40.	1
5gb.	1
:	1
<!--	42
</acl-administer-jobs>	1
</acl-submit-job>	1
</body>	1
</configuration>	18
</description>	58
</html>	1
</properties>	2
</property>	98
</queue>	3
</queues>	1
</table>	1
</tr>	2
</xsl:for-each>	1
</xsl:stylesheet>	1
</xsl:template>	1
<?xml	13
<?xml-stylesheet	7
<LEVEL>,RMSUMMARY	1
<acl-administer-jobs>	1
<acl-submit-job>	1
<body>	1
<configuration>	18
<description>	41
<description>ACL	21
<description>Abase	3
<description>Default	1
<description>Keystore	2
<description>Must	2
<description>Optional.	8
<description>Truststore	4
<html>	1
<name>default.key.acl.DECRYPT_EEK</name>	1
<name>default.key.acl.GENERATE_EEK</name>	1
<name>default.key.acl.MANAGEMENT</name>	1
<name>default.key.acl.READ</name>	1
<name>default</name>	1
<name>dfs.datanode.data.dir</name>	3
<name>dfs.namenode.name.dir</name>	3
<name>dfs.namenode.secondary.http-address</name>	1
<name>dfs.replication</name>	3
<name>fs.defaultFS</name>	3
<name>hadoop.kms.acl.CREATE</name>	1
<name>hadoop.kms.acl.DECRYPT_EEK</name>	1
<name>hadoop.kms.acl.DELETE</name>	1
<name>hadoop.kms.acl.GENERATE_EEK</name>	1
<name>hadoop.kms.acl.GET</name>	1
<name>hadoop.kms.acl.GET_KEYS</name>	1
<name>hadoop.kms.acl.GET_METADATA</name>	1
<name>hadoop.kms.acl.ROLLOVER</name>	1
<name>hadoop.kms.acl.SET_KEY_MATERIAL</name>	1
<name>hadoop.kms.audit.aggregation.window.ms</name>	1
<name>hadoop.kms.authentication.kerberos.keytab</name>	1
<name>hadoop.kms.authentication.kerberos.name.rules</name>	1
<name>hadoop.kms.authentication.kerberos.principal</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.auth.type</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.connection.string</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.keytab</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.kerberos.principal</name>	1
<name>hadoop.kms.authentication.signer.secret.provider.zookeeper.path</name>	1
<name>hadoop.kms.authentication.signer.secret.provider</name>	1
<name>hadoop.kms.authentication.type</name>	1
<name>hadoop.kms.cache.enable</name>	1
<name>hadoop.kms.cache.timeout.ms</name>	1
<name>hadoop.kms.current.key.cache.timeout.ms</name>	1
<name>hadoop.kms.key.provider.uri</name>	1
<name>hadoop.security.keystore.JavaKeyStoreProvider.password</name>	1
<name>hadoop.tmp.dir</name>	3
<name>mapreduce.framework.name</name>	1
<name>q1</name>	1
<name>q2</name>	1
<name>security.admin.operations.protocol.acl</name>	1
<name>security.applicationclient.protocol.acl</name>	1
<name>security.applicationhistory.protocol.acl</name>	1
<name>security.applicationmaster.protocol.acl</name>	1
<name>security.client.datanode.protocol.acl</name>	1
<name>security.client.protocol.acl</name>	1
<name>security.containermanagement.protocol.acl</name>	1
<name>security.datanode.protocol.acl</name>	1
<name>security.ha.service.protocol.acl</name>	1
<name>security.inter.datanode.protocol.acl</name>	1
<name>security.job.client.protocol.acl</name>	1
<name>security.job.task.protocol.acl</name>	1
<name>security.mrhs.client.protocol.acl</name>	1
<name>security.namenode.protocol.acl</name>	1
<name>security.qjournal.service.protocol.acl</name>	1
<name>security.refresh.policy.protocol.acl</name>	1
<name>security.refresh.user.mappings.protocol.acl</name>	1
<name>security.resourcelocalizer.protocol.acl</name>	1
<name>security.resourcemanager-administration.protocol.acl</name>	1
<name>security.resourcetracker.protocol.acl</name>	1
<name>security.zkfc.protocol.acl</name>	1
<name>ssl.client.keystore.keypassword</name>	1
<name>ssl.client.keystore.location</name>	1
<name>ssl.client.keystore.password</name>	1
<name>ssl.client.keystore.type</name>	1
<name>ssl.client.truststore.location</name>	1
<name>ssl.client.truststore.password</name>	1
<name>ssl.client.truststore.reload.interval</name>	1
<name>ssl.client.truststore.type</name>	1
<name>ssl.server.keystore.keypassword</name>	1
<name>ssl.server.keystore.location</name>	1
<name>ssl.server.keystore.password</name>	1
<name>ssl.server.keystore.type</name>	1
<name>ssl.server.truststore.location</name>	1
<name>ssl.server.truststore.password</name>	1
<name>ssl.server.truststore.reload.interval</name>	1
<name>ssl.server.truststore.type</name>	1
<name>yarn.nodemanager.aux-services</name>	1
<name>yarn.resourcemanager.hostname</name>	1
<name>yarn.scheduler.capacity.maximum-am-resource-percent</name>	1
<name>yarn.scheduler.capacity.maximum-applications</name>	1
<name>yarn.scheduler.capacity.node-locality-delay</name>	1
<name>yarn.scheduler.capacity.queue-mappings-override.enable</name>	1
<name>yarn.scheduler.capacity.queue-mappings</name>	1
<name>yarn.scheduler.capacity.resource-calculator</name>	1
<name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>	1
<name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>	1
<name>yarn.scheduler.capacity.root.default.capacity</name>	1
<name>yarn.scheduler.capacity.root.default.maximum-capacity</name>	1
<name>yarn.scheduler.capacity.root.default.state</name>	1
<name>yarn.scheduler.capacity.root.default.user-limit-factor</name>	1
<name>yarn.scheduler.capacity.root.queues</name>	1
<properties>	2
<property	2
<property>	98
<queue>	3
<queues>	1
<state>running</state>	1
<table	1
<td><a	1
<td><xsl:value-of	2
<td>description</td>	1
<td>name</td>	1
<td>value</td>	1
<tr>	2
<value>#HOSTNAME#:#PORT#,...</value>	1
<value>${user.home}/kms.keytab</value>	1
<value>*</value>	36
<value>/etc/hadoop/conf/kms.keytab</value>	1
<value>/hadoop-kms/hadoop-auth-signature-secret</value>	1
<value>0.1</value>	1
<value>10000</value>	4
<value>100</value>	2
<value>1</value>	4
<value>30000</value>	1
<value>40</value>	1
<value>600000</value>	1
<value></value>	11
<value>DEFAULT</value>	1
<value>HTTP/localhost</value>	1
<value>Master:50090</value>	1
<value>Master</value>	1
<value>RUNNING</value>	1
<value>default</value>	1
<value>false</value>	1
<value>file:/usr/local/hadoop/tmp/dfs/data</value>	3
<value>file:/usr/local/hadoop/tmp/dfs/name</value>	3
<value>file:/usr/local/hadoop/tmp</value>	3
<value>hdfs://Master:9000</value>	1
<value>hdfs://localhost:9000</value>	2
<value>jceks://file@/${user.home}/kms.keystore</value>	1
<value>jks</value>	4
<value>kerberos</value>	1
<value>kms/#HOSTNAME#</value>	1
<value>mapreduce_shuffle</value>	1
<value>none</value>	1
<value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>	1
<value>random</value>	1
<value>simple</value>	1
<value>true</value>	1
<value>yarn</value>	1
<xsl:for-each	1
<xsl:output	1
<xsl:stylesheet	1
<xsl:template	1
=	4
@echo	3
@rem	72
A	22
ACL	36
ACL,	2
ACLs	4
ANY	25
ASF	12
AWS	1
Add	1
Admin	2
AdminOperationsProtocol.	1
Advanced	1
All	3
Apache	37
AppSummaryLogging	1
Appender	7
Application	2
ApplicationClientProtocol,	1
ApplicationHistoryProtocol,	1
ApplicationMaster	1
ApplicationMasterProtocol,	1
ApplicationMasters	2
Audit	1
Authentication	2
Automatically	2
BASIS,	25
Backend	1
Below	1
BlockManager	1
By	1
CAN	1
CATALINA_OPTS=	2
CLASSPATH	2
CONDITIONS	25
CPU	1
CREATE	1
Cache	1
Cached	1
Can	1
CapacityScheduler	1
ClientDatanodeProtocol,	1
ClientProtocol,	1
Command	2
Complementary	1
Configuration	17
ContainerManagementProtocol	1
Controller	1
Counter	1
CryptoExtension	2
Currently,	1
Custom	1
DECRYPT_EEK	1
DN.	2
Daily	2
DatanodeProtocol,	1
Date	2
Debugging	2
Default	9
DefaultResourceCalculator	1
Defaults	1
Define	2
DistributedFileSystem.	1
DominantResourceCalculator	1
Duplicate	1
Embedded	2
Empty	2
Event	1
EventCounter	1
Expiry	2
Extra	4
Failover	1
File	2
FileSystem	1
For	24
Foundation	12
GENERATE_EEK	1
GET	2
Ganglia	7
HAAdmin	1
HADOOP_CLASSPATH	1
HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f	1
HADOOP_CLASSPATH=$f	1
HADOOP_CLASSPATH=%HADOOP_CLASSPATH%;%HADOOP_HOME%\contrib\capacity-scheduler\*.jar	1
HADOOP_CLASSPATH=%HADOOP_HOME%\contrib\capacity-scheduler\*.jar	1
HADOOP_CLASSPATH=/usr/lib/jvm/java-8-oracle/lib/tools.jar	1
HADOOP_CLIENT_OPTS="-Xmx512m	1
HADOOP_CLIENT_OPTS=-Xmx512m	1
HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native	1
HADOOP_CONF_DIR=	1
HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop	1
HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS	1
HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS	1
HADOOP_HEAPSIZE=	2
HADOOP_HOME=/usr/local/hadoop	1
HADOOP_IDENT_STRING=$USER	1
HADOOP_IDENT_STRING=%USERNAME%	1
HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData	1
HADOOP_JHS_LOGGER=INFO,RFA	1
HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000	2
HADOOP_JOB_HISTORYSERVER_OPTS=	1
HADOOP_LOG_DIR=${HADOOP_LOG_DIR}/$USER	1
HADOOP_LOG_DIR=%HADOOP_LOG_DIR%\%USERNAME%	1
HADOOP_MAPRED_IDENT_STRING=	1
HADOOP_MAPRED_LOG_DIR=""	1
HADOOP_MAPRED_NICENESS=	1
HADOOP_MAPRED_PID_DIR=	1
HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA	2
HADOOP_MOVER_OPTS=""	1
HADOOP_NAMENODE_INIT_HEAPSIZE=""	2
HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS}	1
HADOOP_NAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER%	1
HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"	1
HADOOP_OPTS	4
HADOOP_OPTS="$HADOOP_OPTS	1
HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"	1
HADOOP_OPTS=%HADOOP_OPTS%	1
HADOOP_PID_DIR=${HADOOP_PID_DIR}	1
HADOOP_PID_DIR=%HADOOP_PID_DIR%	1
HADOOP_PORTMAP_OPTS="-Xmx512m	1
HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS}	1
HADOOP_SECONDARYNAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER%	1
HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}	1
HADOOP_SECURE_DN_LOG_DIR=%HADOOP_LOG_DIR%\%HADOOP_HDFS_USER%	1
HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}	1
HADOOP_SECURE_DN_PID_DIR=%HADOOP_PID_DIR%	1
HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}	1
HADOOP_SECURE_DN_USER=%HADOOP_SECURE_DN_USER%	1
HADOOP_SECURITY_LOGGER	1
HADOOP_SECURITY_LOGGER=INFO,RFAS	1
HADOOP_YARN_USER	1
HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}	1
HADOOP_YARN_USER=%yarn%	1
HAService	1
HDFS	4
HDFS_AUDIT_LOGGER	1
HDFS_AUDIT_LOGGER=INFO,NullAppender	1
HEAP	1
HH:mm:ss}	2
HS	1
HSClientProtocol,	1
HTTP	4
HTTPFS_ADMIN_PORT=`expr	1
HTTPFS_HTTP_HOSTNAME=`hostname	1
HTTPFS_HTTP_PORT=14000	1
HTTPFS_LOG=${HTTPFS_HOME}/logs	1
HTTPFS_SSL_ENABLED=false	1
HTTPFS_SSL_KEYSTORE_FILE=${HOME}/.keystore	1
HTTPFS_SSL_KEYSTORE_PASS=password	1
HTTPFS_TEMP=${HTTPFS_HOME}/temp	1
Hadoop	2
Hadoop-specific	2
Heapsize	3
Here	1
History	1
Http	1
HttpFS	8
HttpFSServer	1
IFS	1
IFS=	1
IS"	25
If	19
In	1
Indicates	2
InterDatanodeProtocol,	1
Irrespective	2
It	2
JAVA=$JAVA_HOME/bin/java	1
JAVA_HEAP_MAX	3
JAVA_HEAP_MAX="-Xmx""$YARN_HEAPSIZE""m"	1
JAVA_HEAP_MAX=-Xmx%YARN_HEAPSIZE%m	1
JAVA_HEAP_MAX=-Xmx1000m	1
JAVA_HOME	3
JAVA_HOME.	2
JAVA_HOME=$JAVA_HOME	1
JAVA_HOME=${JAVA_HOME}	1
JAVA_HOME=%JAVA_HOME%	1
JAVA_HOME=/home/y/libexec/jdk1.6.0/	2
JAVA_HOME=/usr/lib/jvm/java-8-oracle	1
JAVA_LIBRARY_PATH	1
JNs	1
JSVC_HOME=${JSVC_HOME}	1
JSVC_HOME=%JSVC_HOME%	1
JVM	3
Java	10
JavaKeyStoreProvider,	1
Jets3t	1
Job	1
JobSummary	1
Jsvc	3
KIND,	25
KMS	17
KMS.	2
KMS_ADMIN_PORT=`expr	1
KMS_HTTP_PORT=16000	1
KMS_LOG=${KMS_HOME}/logs	1
KMS_MAX_THREADS=1000	1
KMS_SSL_KEYSTORE_FILE=${HOME}/.keystore	1
KMS_SSL_KEYSTORE_PASS=password	1
KMS_TEMP=${KMS_HOME}/temp	1
Kerberos	6
KeyProvider	2
KeyProvider.	2
LICENSE	10
License	75
License,	25
License.	50
Licensed	25
Log	3
LogLevel	2
LogMessage	2
LoggerName	2
Logging	2
Logs	1
MANAGEMENT	1
MB.	5
MR	2
MRClientProtocol,	1
Manager	2
Map/Reduce	2
Maximum	2
Memory	1
Memory,	1
Metrics.	1
Modifications	1
Mover	1
Mover.	1
Must	4
NN	3
NOT**	1
NOTE:	2
NOTICE	12
Name	1
NameNode.	1
NamenodeProtocol,	1
Node	1
NodeManager	4
NodeManager.	1
Note	2
Null	1
Number	1
OF	25
OR	25
On	2
Only!	1
Options	1
Otherwise	2
Parameter	1
Path	1
Pattern	4
Pick	2
Protocols	1
Put	5
QJournalProtocol,	1
Queue	1
QuorumJournalManager	1
READ	1
RM,	1
ROLLOVER	1
RUNNING	1
RefreshAuthorizationPolicyProtocol,	1
RefreshUserMappingsProtocol.	1
Request	1
Required.	1
Requires	1
Resource	1
ResourceCalculator	1
ResourceLocalizer	2
ResourceManager	8
ResourceManager.	1
ResourceManagerAdministrationProtocol,	1
ResourceTrackerProtocol,	1
Resources	1
Rolling	3
Rollver	1
Rules	1
S3A	1
SASL	2
SDK	1
SPNEGO	1
SSL	9
STOPPED.	1
Security	2
See	48
Sends	1
Server	2
Set	6
Settings	2
Setup	2
Site	1
Slave1	1
Software	12
Specifies	2
Specify	6
Specifying	1
State	2
Summary	3
System	4
Tag	1
TaskLog	1
TaskUmbilicalProtocol,	1
The	96
These	3
This	10
Threshold	1
To	1
Tomcat	3
Typically	2
URI	1
Uncomment	2
Unless	25
Use	1
Used	2
User	2
Users	1
Version	25
WARRANTIES	25
WITHOUT	25
When	3
Where	5
Whether	1
XML	1
Xmx	3
YARN	5
YARN_CONF_DIR	1
YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}"	1
YARN_CONF_DIR=%HADOOP_YARN_HOME%\conf	1
YARN_HEAPMAX	6
YARN_HEAPSIZE	1
YARN_HEAPSIZE=1000	1
YARN_LOGFILE	1
YARN_LOGFILE='yarn.log'	1
YARN_LOGFILE=yarn.log	1
YARN_LOG_DIR	1
YARN_LOG_DIR="$HADOOP_YARN_HOME/logs"	1
YARN_LOG_DIR=%HADOOP_YARN_HOME%\logs	1
YARN_NODEMANAGER_HEAPSIZE=1000	1
YARN_NODEMANAGER_OPTS.	1
YARN_NODEMANAGER_OPTS=	1
YARN_OPTS	7
YARN_OPTS="$YARN_OPTS	10
YARN_OPTS=%YARN_OPTS%	11
YARN_POLICYFILE	1
YARN_POLICYFILE="hadoop-policy.xml"	1
YARN_POLICYFILE=hadoop-policy.xml	1
YARN_RESOURCEMANAGER_HEAPSIZE=1000	1
YARN_RESOURCEMANAGER_OPTS.	1
YARN_RESOURCEMANAGER_OPTS=	1
YARN_ROOT_LOGGER	1
YARN_ROOT_LOGGER=INFO,console	1
YARN_TIMELINESERVER_HEAPSIZE=1000	1
YARN_TIMELINESERVER_OPTS.	1
Yarn	1
You	34
ZK	1
ZNode	1
Zookeeper	3
Zookeeper.	2
[	8
[%X{hostname}][%X{user}:%X{doAs}]	2
[prefix].[source|sink].[instance].[options]	1
[u|g]:[name]:[queue_name][,next	1
];	8
a	91
above	3
absolute	1
accept	2
access	1
accompanying	10
acl	1
acls	5
act	1
active	1
additional	12
admin	2
administer	1
administrators	3
affects	2
after	3
aggregated	2
aggregation	1
agreed	25
agreements.	12
all	30
allow	1
allowed	2
allowed.</description>	18
allowed.system.users=##comma	1
allows	2
along	1
also	1
amount	2
an	31
and	94
and/or	3
any	4
app	2
appended	5
appender	3
appender)	1
applicable	25
application	1
applications	4
applications.	1
applies	2
appropriately	1
approximately	1
are	44
args	1
as	15
assign	1
at	32
attack.	2
attempts	1
audit	4
authentication	7
authorization	5
backing	3
backup	1
banned.users=#comma	1
be	47
behaviour	1
below	1
best	2
bind	1
blank.	18
block	4
border="1">	1
by	91
cache	2
cache,	2
cached	1
called	1
can	14
cannot	1
cap	1
capacity	1
capacity-scheduler.	2
capacity.</description>	1
case	1
change	2
changes	1
changing	1
check	5
checked	1
child	1
client	2
client-to-datanode	1
clients	5
cluster	5
cluster,	1
code	1
comma	1
comma-separated	18
commands	3
commands.	2
commas.	1
communciate	2
communicate	10
compare	2
compliance	25
concurrent	1
configs	1
configuration	9
configuration,	2
configuration.	2
configured	7
connect	2
connection	1
console	1
consulting	1
contain	2
containers.	1
context	18
contributor	12
controls	1
cookie	2
cookies	1
copy	25
copyright	12
correctly	3
count	1
counts	1
create-key	1
creating	1
credentials	2
current	1
daemons	2
daemons.	3
data	7
datanode	2
datanodes	2
datanodes,	2
datanodes.	1
decryptEncryptedKey	1
default	27
default.	10
defined	20
defined.	4
defining	1
delete-key	1
deleted	1
deprecation	1
details	1
dfs,	2
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
dfs.class=org.apache.hadoop.metrics.spi.NullContext	1
dfs.period=10	1
dfs.servers=localhost:8649	1
dfsadmin	1
different	5
directories.</description>	3
directory	9
directory)	1
distcp	2
distcp.	2
distributed	64
do	6
document.	1
does	1
dominant-resource	1
done	1
dropping	2
e.g.	18
each	6
echo	2
edit	1
either	32
element	1
element.	1
elements.	2
else	2
enable	2
enabled	2
enabled,	1
encoding="UTF-8"?>	5
end	1
endpoint.	1
environment	6
environment.	2
envvars	1
etc)	2
etc.	3
events	2
events.	1
example	1
example,	5
except	25
exist	1
exit	1
explicitly	4
export	45
express	25
f	1
false	1
false.	1
feature	1
fi	8
file	63
file,	2
file.	16
filename	1
filename)	1
filenames	1
files	8
files)	1
flags	3
flushed	1
following	7
for	149
format	5
format,	4
format:	2
from	7
from.	1
fsck,	2
ganglia	6
generateEncryptedKey	1
generation	1
generic	1
get-current-key	1
get-key-metadata	1
get-key-version	1
get-keys	1
get-keys-metadata	1
getCurrentKey	2
getKeyVersion	1
getKeyVersion,	1
getMetadata,	1
getMetadata.	1
governing	25
group	36
group1,group2	2
groups	2
hadoop	4
hadoop-env.sh	1
hadoop.	3
hadoop.log.dir	1
hadoop.log.dir=.	1
hadoop.log.file=hadoop.log	1
hadoop.log.maxbackupindex=20	1
hadoop.log.maxfilesize=256MB	1
hadoop.mapreduce.jobsummary.log.file	1
hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log	1
hadoop.mapreduce.jobsummary.log.maxbackupindex=20	1
hadoop.mapreduce.jobsummary.log.maxfilesize=256MB	1
hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}	1
hadoop.mapreduce.jobsummary.logger=INFO,JSA	1
hadoop.root.logger=INFO,console	1
hadoop.security.log.file=SecurityAuth-${user.name}.audit	1
hadoop.security.log.maxbackupindex=20	1
hadoop.security.log.maxfilesize=256MB	1
hadoop.security.logger=INFO,NullAppender	1
hadoop.tasklog.iscleanup=false	1
hadoop.tasklog.logsRetainHours=12	1
hadoop.tasklog.noKeepSplits=4	1
hadoop.tasklog.purgeLogSplits=true	1
hadoop.tasklog.taskid=null	1
hadoop.tasklog.totalLogFileSize=100	1
handled	1
handler	1
has	1
hdfs	1
hdfs.audit.log.maxbackupindex=20	1
hdfs.audit.log.maxfilesize=256MB	1
hdfs.audit.logger=INFO,NullAppender	1
heap	2
heapsize	1
here	2
here.	4
hierarchical	2
his/her	1
history	1
hostname	1
hostnames	1
hot-reloaded	1
how	1
href="configuration.xsl"?>	7
http://www.apache.org/licenses/LICENSE-2.0	25
httpfs	4
httpfsaudit	1
i.e.	2
if	32
implementation	5
implied.	25
in	94
in-effect.	1
include	1
information	12
insert	2
instance	3
instances	1
instances,	1
inter-datanode	1
interval,	2
irrespective	1
is	121
it	9
it.	1
its	2
java	3
javadoc	1
job	6
jobs	10
jobs,	1
jobs.	2
jsvc	2
jvm	3
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
jvm.period=10	1
jvm.servers=localhost:8649	1
key	10
key.	1
key="capacity"	1
key="user-limit"	1
keys	1
keystore	9
keytab	2
killing	1
kms	2
kms-audit	1
language	25
last	1
law	25
leaf	2
level	6
levels	2
library	1
license	12
licenses	12
like	3
limit	1
limitations	25
line	1
links	1
list	44
location	2
log	14
log4j.additivity.kms-audit=false	1
log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false	1
log4j.additivity.org.apache.hadoop.mapred.AuditLogger=false	1
log4j.additivity.org.apache.hadoop.mapred.JobInProgress$JobSummary=false	1
log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false	1
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd	1
log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}	1
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd	1
log4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}	1
log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout	1
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter	1
log4j.appender.JSA.File=${hadoop.log.dir}/${hadoop.mapreduce.jobsummary.log.file}	1
log4j.appender.JSA.MaxBackupIndex=${hadoop.mapreduce.jobsummary.log.maxbackupindex}	1
log4j.appender.JSA.MaxFileSize=${hadoop.mapreduce.jobsummary.log.maxfilesize}	1
log4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd	1
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.JSA=org.apache.log4j.RollingFileAppender	1
log4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log	1
log4j.appender.MRAUDIT.MaxBackupIndex=${mapred.audit.log.maxbackupindex}	1
log4j.appender.MRAUDIT.MaxFileSize=${mapred.audit.log.maxfilesize}	1
log4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout	1
log4j.appender.MRAUDIT=org.apache.log4j.RollingFileAppender	1
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender	1
log4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}	1
log4j.appender.RFA.MaxBackupIndex=${hadoop.log.maxbackupindex}	1
log4j.appender.RFA.MaxFileSize=${hadoop.log.maxfilesize}	1
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFA=org.apache.log4j.RollingFileAppender	1
log4j.appender.RFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log	1
log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}	1
log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}	1
log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender	1
log4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}	1
log4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}	1
log4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}	1
log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RFAS=org.apache.log4j.RollingFileAppender	1
log4j.appender.RMSUMMARY.File=${hadoop.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}	1
log4j.appender.RMSUMMARY.MaxBackupIndex=20	1
log4j.appender.RMSUMMARY.MaxFileSize=256MB	1
log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout	1
log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender	1
log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}	1
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.TLA.layout=org.apache.log4j.PatternLayout	1
log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}	1
log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}	1
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender	1
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd	1
log4j.appender.console.layout=org.apache.log4j.PatternLayout	1
log4j.appender.console.target=System.err	1
log4j.appender.console=org.apache.log4j.ConsoleAppender	1
log4j.appender.httpfs.Append=true	1
log4j.appender.httpfs.DatePattern='.'yyyy-MM-dd	1
log4j.appender.httpfs.File=${httpfs.log.dir}/httpfs.log	1
log4j.appender.httpfs.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.httpfs.layout=org.apache.log4j.PatternLayout	1
log4j.appender.httpfs=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.httpfsaudit.Append=true	1
log4j.appender.httpfsaudit.DatePattern='.'yyyy-MM-dd	1
log4j.appender.httpfsaudit.File=${httpfs.log.dir}/httpfs-audit.log	1
log4j.appender.httpfsaudit.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.httpfsaudit.layout=org.apache.log4j.PatternLayout	1
log4j.appender.httpfsaudit=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.kms-audit.Append=true	1
log4j.appender.kms-audit.DatePattern='.'yyyy-MM-dd	1
log4j.appender.kms-audit.File=${kms.log.dir}/kms-audit.log	1
log4j.appender.kms-audit.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.kms-audit.layout=org.apache.log4j.PatternLayout	1
log4j.appender.kms-audit=org.apache.log4j.DailyRollingFileAppender	1
log4j.appender.kms.Append=true	1
log4j.appender.kms.DatePattern='.'yyyy-MM-dd	1
log4j.appender.kms.File=${kms.log.dir}/kms.log	1
log4j.appender.kms.layout.ConversionPattern=%d{ISO8601}	1
log4j.appender.kms.layout=org.apache.log4j.PatternLayout	1
log4j.appender.kms=org.apache.log4j.DailyRollingFileAppender	1
log4j.category.SecurityLogger=${hadoop.security.logger}	1
log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR	1
log4j.logger.com.amazonaws=ERROR	1
log4j.logger.com.sun.jersey.server.wadl.generators.WadlGeneratorJAXBGrammarGenerator=OFF	1
log4j.logger.httpfsaudit=INFO,	1
log4j.logger.kms-audit=INFO,	1
log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN	1
log4j.logger.org.apache.hadoop.conf=ERROR	1
log4j.logger.org.apache.hadoop.fs.http.server=INFO,	1
log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN	1
log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}	1
log4j.logger.org.apache.hadoop.lib=INFO,	1
log4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}	1
log4j.logger.org.apache.hadoop.mapred.JobInProgress$JobSummary=${hadoop.mapreduce.jobsummary.logger}	1
log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}	1
log4j.logger.org.apache.hadoop=INFO	1
log4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR	1
log4j.rootLogger=${hadoop.root.logger},	1
log4j.rootLogger=ALL,	1
log4j.threshold=ALL	1
logger	2
logger.	1
logging	4
logs	2
logs.</description>	1
loops	1
manage	1
manager	3
map	2
mapping	1
mapping]*	1
mappings	1
mappings.	1
mapred	1
mapred.audit.log.maxbackupindex=20	1
mapred.audit.log.maxfilesize=256MB	1
mapred.audit.logger=INFO,NullAppender	1
mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
mapred.class=org.apache.hadoop.metrics.spi.NullContext	1
mapred.period=10	1
mapred.servers=localhost:8649	1
mapreduce.cluster.acls.enabled	3
mapreduce.cluster.administrators	2
maps	1
masters	1
match="configuration">	1
material	3
max	3
maximum	4
may	54
means	18
message	1
messages	2
metadata	1
method="html"/>	1
metrics	3
midnight	1
might	1
milliseconds.	4
min.user.id=1000#Prevent	1
missed	1
modified.	1
modify	1
modifying	1
more	12
mradmin	1
ms)	1
multi-dimensional	1
multiple	4
must	1
name	3
name.	1
name="{name}"><xsl:value-of	1
namenode	1
namenode-metrics.out	1
namenode.	2
namenode.</description>	1
namenode:	1
names.	19
nesting	1
new	2
no	4
nodes	2
nodes.	2
non-privileged	2
normal	1
not	52
null	5
number	6
numerical	3
obtain	25
of	139
off	4
on	31
one	15
one:	2
only	9
operation.	3
operations	8
operations.	9
opportunities	1
option	6
optional.	2
options	11
options.	2
or	73
ordinary	1
org.apache.hadoop.metrics2	1
other	4
other.	6
others	2
overridden	4
override	5
overrides	5
owner	1
ownership.	12
package-info.java	1
parameters	4
parent	1
part	2
password	3
path	2
pending	1
per	1
percent	1
percentage	1
period	1
period,	1
permissions	25
picked	3
pid	3
place	1
please	1
policy	3
port	5
ports	2
ports.	2
potential	2
preferred	3
prefix.	1
present,	1
principal	4
principal.	1
printed	1
priorities.	1
priority	1
privileged	2
privileges	1
privileges.	1
properties	7
property	12
protocol	6
protocol,	2
protocol.	2
provide	3
q1.	1
q2	2
q2.	1
quashed	1
query	1
queue	12
queue).	1
queue,	1
queue.	10
queues	9
queues,	1
queues.	3
rack	1
rack-local	1
recovery.	1
reduce	2
refresh	2
regarding	12
reload	2
remote	2
representing	3
required	30
resolve	2
resources	2
response.	2
restore	1
retrieve	1
return	1
returned	2
rolling	1
rollover-key	1
root	2
rootlogger	1
rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
rpc.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
rpc.class=org.apache.hadoop.metrics.spi.NullContext	1
rpc.period=10	1
rpc.servers=localhost:8649	1
run	10
running	3
running,	1
running.	1
runs	3
runtime	2
same	1
sample	1
sampling	2
scale	3
schedule	1
scheduler.	1
schedulers,	1
scheduling	2
secondary	1
seconds	1
seconds).	2
secret	3
secure	7
security	1
segment	1
select="description"/></td>	1
select="name"/></a></td>	1
select="property">	1
select="value"/></td>	1
send	1
sending	1
separate	2
separated	20
separated.	1
server	3
service	2
service-level	2
set	60
set."	1
sets	2
setting	7
setup	1
severity	1
should	6
sign	1
signature	2
similar	3
single	1
sinks	1
site-specific	5
sizes	1
so	3
softlink	1
software	25
some	2
sometimes	1
source	2
space	1
space),	2
spaces	1
special	19
specific	34
specification.	1
specified	13
specified,	3
specified.	6
specifiying	1
specify	3
specifying	2
split	1
stand-by	1
start	3
started	2
starting	3
state	4
states	1
status	2
stopped,	1
store	1
stored	2
stored.	7
string	3
string,	1
submission	1
submit	2
submitting	1
such	1
summary	5
super-users	1
support	2
supported	1
supports	1
supportsparse	1
suppress	1
symlink	2
syntax	1
syntax:	1
system	3
tag	1
tags	3
target	1
tasks	2
tasktracker.	1
template	1
temporary	5
than	1
that	19
the	377
them	1
then	8
there	2
therefore	3
this	80
threads	1
time	4
timeline	2
timestamp.	1
to	164
top	1
traffic.	1
transfer	4
true.	3
turn	1
two.	3
type	1
type,	1
type="text/xsl"	7
typically	1
u:%user:%user	1
ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext	1
ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext31	1
ugi.class=org.apache.hadoop.metrics.spi.NullContext	1
ugi.period=10	1
ugi.servers=localhost:8649	1
uncommented	1
under	87
unset	1
up	2
updating	1
usage	2
use	31
use,	2
use.	4
used	36
used.	3
user	48
user.	2
user1,user2	2
user?	1
users	27
users,wheel".	18
uses	2
using	14
value	45
value="20"/>	1
value="30"/>	1
values	4
variable	4
variables	4
version	1
version="1.0"	5
version="1.0">	1
version="1.0"?>	8
via	3
view,	1
viewing	1
w/	1
want	1
warnings.	1
when	9
where	4
which	7
while	1
who	6
will	23
window	1
window,	1
with	60
within	4
without	1
work	12
writing,	25
written	2
xmlns:xsl="http://www.w3.org/1999/XSL/Transform"	1
yarn.nodemanager.linux-container-executor.group	1
yarn.nodemanager.linux-container-executor.group=#configured	1
yarn.server.resourcemanager.appsummary.log.file	1
yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log	1
yarn.server.resourcemanager.appsummary.logger	2
yarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}	1
you	29
